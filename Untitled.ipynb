{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.env_FX_trade import ForexEnvTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.policy_network import PolicyNetworkBase, DPG_PolicyNetworkLSTM, DDPG_PolicyNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# import from local file\n",
    "from model.policy_network import PolicyNetworkBase, DPG_PolicyNetworkLSTM, DDPG_PolicyNetwork\n",
    "\n",
    "from env.env_FX_trade import ForexEnvTrain\n",
    "\n",
    "from preprocess_data.preprocessors import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = int((self.position + 1) % self.capacity)  # as a ring buffer\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch)) # stack for each element\n",
    "        ''' \n",
    "        the * serves as unpack: sum(a,b) <=> batch=(a,b), sum(*batch) ;\n",
    "        zip: a=[1,2], b=[2,3], zip(a,b) => [(1, 2), (2, 3)] ;\n",
    "        the map serves as mapping the function on each list element: map(square, [2,3]) => [4,9] ;\n",
    "        np.stack((1,2)) => array([1, 2])\n",
    "        '''\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim, init_w=3e-3):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.action_dim=output_dim\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, output_dim) # output dim = dim of action\n",
    "\n",
    "        # weights initialization\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "    \n",
    "\n",
    "    def forward(self, state):\n",
    "        activation=F.relu\n",
    "        x = activation(self.linear1(state)) \n",
    "        x = activation(self.linear2(x))\n",
    "        # x = F.tanh(self.linear3(x)).clone() # need clone to prevent in-place operation (which cause gradients not be drived)\n",
    "        x = self.linear3(x) # for simplicity, no restriction on action range\n",
    "\n",
    "        return x\n",
    "\n",
    "    def select_action(self, state, noise_scale=1.0):\n",
    "        '''\n",
    "        select action for sampling, no gradients flow, noisy action, return .cpu\n",
    "        '''\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device) # state dim: (N, dim of state)\n",
    "        normal = Normal(0, 1)\n",
    "        action = self.forward(state)\n",
    "        noise = noise_scale * normal.sample(action.shape).to(device)\n",
    "        action+=noise\n",
    "        return action.detach().cpu().numpy()[0]\n",
    "\n",
    "    def sample_action(self, action_range=1.):\n",
    "        normal = Normal(0, 1)\n",
    "        random_action=action_range*normal.sample( (self.action_dim,) )\n",
    "\n",
    "        return random_action.cpu().numpy()\n",
    "\n",
    "\n",
    "    def evaluate_action(self, state, noise_scale=0.0):\n",
    "        '''\n",
    "        evaluate action within GPU graph, for gradients flowing through it, noise_scale controllable\n",
    "        '''\n",
    "        normal = Normal(0, 1)\n",
    "        action = self.forward(state)\n",
    "        # action = torch.tanh(action)\n",
    "        noise = noise_scale * normal.sample(action.shape).to(device)\n",
    "        action+=noise\n",
    "        return action\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, init_w=3e-3):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], 1) # the dim 0 is number of samples\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = self.linear3(x)\n",
    "        return x\n",
    "\n",
    "class DDPG():\n",
    "    def __init__(self, replay_buffer, state_dim, action_dim, hidden_dim):\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.qnet = QNetwork(state_dim+action_dim, hidden_dim).to(device)\n",
    "        self.target_qnet = QNetwork(state_dim+action_dim, hidden_dim).to(device)\n",
    "        self.policy_net = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.target_policy_net = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        print('Q network: ', self.qnet)\n",
    "        print('Policy network: ', self.policy_net)\n",
    "\n",
    "        for target_param, param in zip(self.target_qnet.parameters(), self.qnet.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "        self.q_criterion = nn.MSELoss()\n",
    "        q_lr=8e-4\n",
    "        policy_lr = 8e-4\n",
    "        self.update_cnt=0\n",
    "\n",
    "        self.q_optimizer = optim.Adam(self.qnet.parameters(), lr=q_lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
    "    \n",
    "    def target_soft_update(self, net, target_net, soft_tau):\n",
    "    # Soft update the target net\n",
    "        for target_param, param in zip(target_net.parameters(), net.parameters()):\n",
    "            target_param.data.copy_(  # copy data value into target parameters\n",
    "                target_param.data * (1.0 - soft_tau) + param.data * soft_tau\n",
    "            )\n",
    "\n",
    "        return target_net\n",
    "\n",
    "    def update(self, batch_size, reward_scale=10.0, gamma=0.99, soft_tau=1e-2, policy_up_itr=10, target_update_delay=3, warmup=True):\n",
    "        self.update_cnt+=1\n",
    "        state, action, reward, next_state, done = self.replay_buffer.sample(batch_size)\n",
    "        # print('sample:', state, action,  reward, done)\n",
    "\n",
    "        state      = torch.FloatTensor(state).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        action     = torch.FloatTensor(action).to(device)\n",
    "        reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)  \n",
    "        done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
    "\n",
    "        predict_q = self.qnet(state, action) # for q \n",
    "        new_next_action = self.target_policy_net.evaluate_action(next_state)  # for q\n",
    "        new_action = self.policy_net.evaluate_action(state) # for policy\n",
    "        predict_new_q = self.qnet(state, new_action) # for policy\n",
    "        target_q = reward+(1-done)*gamma*self.target_qnet(next_state, new_next_action)  # for q\n",
    "        # reward = reward_scale * (reward - reward.mean(dim=0)) /reward.std(dim=0) # normalize with batch mean and std\n",
    "\n",
    "        # train qnet\n",
    "        q_loss = self.q_criterion(predict_q, target_q.detach())\n",
    "        self.q_optimizer.zero_grad()\n",
    "        q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        # train policy_net\n",
    "        policy_loss = -torch.mean(predict_new_q)\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "\n",
    "            \n",
    "        # update the target_qnet\n",
    "        if self.update_cnt%target_update_delay==0:\n",
    "            self.target_qnet=self.target_soft_update(self.qnet, self.target_qnet, soft_tau)\n",
    "            self.target_policy_net=self.target_soft_update(self.policy_net, self.target_policy_net, soft_tau)\n",
    "\n",
    "        return q_loss.detach().cpu().numpy(), policy_loss.detach().cpu().numpy()\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.qnet.state_dict(), path+'_q')\n",
    "        torch.save(self.target_qnet.state_dict(), path+'_target_q')\n",
    "        torch.save(self.policy_net.state_dict(), path+'_policy')\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.qnet.load_state_dict(torch.load(path+'_q'))\n",
    "        self.target_qnet.load_state_dict(torch.load(path+'_target_q'))\n",
    "        self.policy_net.load_state_dict(torch.load(path+'_policy'))\n",
    "        self.qnet.eval()\n",
    "        self.target_qnet.eval()\n",
    "        self.policy_net.eval()\n",
    "\n",
    "def plot(rewards):\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(rewards)\n",
    "    plt.savefig('ddpg.png')\n",
    "    # plt.show()\n",
    "    plt.clf()\n",
    "\n",
    "class NormalizedActions(gym.ActionWrapper): # gym env wrapper\n",
    "    def _action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def _reverse_action(self, action):\n",
    "        low  = self.action_space.low\n",
    "        high = self.action_space.high\n",
    "        \n",
    "        action = 2 * (action - low) / (high - low) - 1\n",
    "        action = np.clip(action, low, high)\n",
    "        \n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = preprocess_data('data/datatrain_FX_01062022_15062022.csv')\n",
    "env = NormalizedActions(ForexEnvTrain(df2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = [-0.54982066,1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.array(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<NormalizedActions<ForexEnvTrain instance>>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-815425c95cb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
